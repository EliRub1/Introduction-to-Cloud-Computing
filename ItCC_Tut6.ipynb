{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOaUrIqj846oyH4zRDMyAcP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EliRub1/Introduction-to-Cloud-Computing/blob/main/ItCC_Tut6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIZYxWHTrwkK"
      },
      "outputs": [],
      "source": [
        "!pip install requests beautifulsoup4\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetch_page(url):\n",
        " response = requests.get(url)\n",
        " if response.status_code == 200:\n",
        "  soup = BeautifulSoup(response.text, 'html.parser')\n",
        "  return soup\n",
        " else:\n",
        "  return None\n",
        "#saves the text only from the given url"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def index_words(soup):\n",
        "  index = {}\n",
        "  words = re.findall(r'\\w+', soup.get_text())\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    if word in index:\n",
        "      index[word] += 1\n",
        "    else:\n",
        "      index[word] = 1\n",
        "  return index\n",
        "\n",
        "  #counts how many times any word is in the text"
      ],
      "metadata": {
        "id": "SpCjENTstF8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(index):\n",
        "  stop_words = {'a', 'an', 'the', 'and', 'or',     \t\t\t\t'in', 'on', 'at'}\n",
        "  for stop_word in stop_words:\n",
        "    if stop_word in index:\n",
        "      del index[stop_word]\n",
        "  return index\n",
        "\n",
        "  #removes the stopwords from the text"
      ],
      "metadata": {
        "id": "jV92Tl5SvcXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "def apply_stemming(index):\n",
        "  stemmer = PorterStemmer()\n",
        "  stemmed_index = {}\n",
        "  for word, count in index.items():\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    if stemmed_word in stemmed_index:\n",
        "      stemmed_index[stemmed_word] += count\n",
        "    else:\n",
        "      stemmed_index[stemmed_word] = count\n",
        "  return stemmed_index\n",
        "\n",
        "  #בודקים את מספר המופעים של מילים עם אותה המשמעות: example => plays,playing,played ==> play"
      ],
      "metadata": {
        "id": "DyeAjLNsv03P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search(query, index):\n",
        "  query_words = re.findall(r'\\w+', query.lower())\n",
        "  results = {}\n",
        "  for word in query_words:\n",
        "    if word in index:\n",
        "      results[word] = index[word]\n",
        "  return results\n",
        "\n",
        "  #checks if the searched word is in the query word(שאילתה) and saves the index num (לצורך הדוגמא ככה רואים אם יש מידע שאנו רוצים בדף..)"
      ],
      "metadata": {
        "id": "nbH3OUgkzIMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_engine(url, query):\n",
        "  soup = fetch_page(url)\n",
        "  if soup is None:\n",
        "     return None\n",
        "  index = index_words(soup)\n",
        "  index = remove_stop_words(index)\n",
        "  index = apply_stemming(index)\n",
        "  results = search(query, index)\n",
        "  return results\n",
        "\n",
        "  #searchs for the query in the given url"
      ],
      "metadata": {
        "id": "zZ9Tq2YR0RQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://en.wikipedia.org/wiki/Bird'\n",
        "query = 'bird'\n",
        "results = search_engine(url, query)\n",
        "print(results)\n",
        "#counts how many time the word  bird is in the wiki page"
      ],
      "metadata": {
        "id": "F0qCgeOk0p5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://en.wikipedia.org/wiki/Bird'\n",
        "query = 'birds wings'\n",
        "results = search_engine(url, query)\n",
        "print(results)"
      ],
      "metadata": {
        "id": "g1HO6V2Q1Ee7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search(query, index):\n",
        "  stemmer = PorterStemmer()\n",
        "  query_words = re.findall(r'\\w+', query.lower())\n",
        "  results = {}\n",
        "  for word in query_words:\n",
        "    word = stemmer.stem(word)\n",
        "    if word in index:\n",
        "      results[word] = index[word]\n",
        "  return results\n",
        "\n",
        "  #change the way the search is made by stemming the words and that counts it."
      ],
      "metadata": {
        "id": "gf8znG9_1oCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://en.wikipedia.org/wiki/Bird'\n",
        "query = 'birds wings'\n",
        "results = search_engine(url, query)\n",
        "print(results)\n",
        "rank=1\n",
        "for word, count in results.items():\n",
        "   rank = rank*1/count\n",
        "rank = 1-rank\n",
        "print(rank)\n",
        "\n",
        "#made a ranking system and gave the page the rank 0.99999 thats a high rank because the words birds+wings are shown a lot in the page"
      ],
      "metadata": {
        "id": "0nQIJ21x7nMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://en.wikipedia.org/wiki/Bird'\n",
        "query = 'collage students'\n",
        "results = search_engine(url, query)\n",
        "print(results)\n",
        "rank=1\n",
        "for word, count in results.items():\n",
        " rank = rank*1/count\n",
        "rank = 1-rank\n",
        "print(rank)\n",
        "# the rank is 0 because there is no collage students mentioned in the page"
      ],
      "metadata": {
        "id": "4Xly3k718Asp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://en.wikipedia.org/wiki/Bird'\n",
        "query = 'owls'\n",
        "results = search_engine(url, query)\n",
        "print(results)\n",
        "rank=1\n",
        "for word, count in results.items():\n",
        " rank = rank*1/count\n",
        "rank = 1-rank\n",
        "print(rank)\n",
        "\n",
        "#good rank because the word owl is in the page +-"
      ],
      "metadata": {
        "id": "XOxgq9dp8KWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://w3.braude.ac.il/?lang=en'\n",
        "query = 'Industry'\n",
        "results = search_engine(url, query)\n",
        "print(results)\n",
        "rank=1\n",
        "for word, count in results.items():\n",
        " rank = rank*1/count\n",
        "rank = 1-rank\n",
        "print(rank)\n",
        "\n",
        "#searching for the word industry in the braude collage site"
      ],
      "metadata": {
        "id": "q5w30f7c84yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://w3.braude.ac.il/?lang=en'\n",
        "query = 'Braude collage'\n",
        "results = search_engine(url, query)\n",
        "print(results)\n",
        "rank=1\n",
        "for word, count in results.items():\n",
        " rank = rank*1/count\n",
        "rank = 1-rank\n",
        "print(rank)\n",
        "\n",
        "#searching for the words braude collage in the braude collage site"
      ],
      "metadata": {
        "id": "DnC-D5Mg87KM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://w3.braude.ac.il/?lang=en'\n",
        "query = 'Galilee center'\n",
        "results = search_engine(url, query)\n",
        "print(results)\n",
        "rank=1\n",
        "for word, count in results.items():\n",
        " rank = rank*1/count\n",
        "rank = 1-rank\n",
        "print(rank)\n",
        "#searching for the words Galilee center in the braude collage site"
      ],
      "metadata": {
        "id": "9bXh3m5X8_71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**engine for multiple pages**"
      ],
      "metadata": {
        "id": "4VI9UOMc-QvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from collections import defaultdict\n",
        "class WikiSearchEngine:\n",
        "  def __init__(self):\n",
        "    \"\"\"Initialize the search engine\"\"\"\n",
        "    self.base_url = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "    self.pages = []\n",
        "    self.word_locations = defaultdict(list) # word -> [(page_id, frequency), ...]\n",
        "    self.stop_words = {'a', 'an', 'the', 'and', 'or', 'in', 'on', 'at', 'to', 'for', 'of', 'with'}\n",
        "\n",
        "  def fetch_wiki_pages(self, topic, num_pages=5):\n",
        "    \"\"\"Fetch Wikipedia pages for given topic\"\"\"\n",
        "    search_params = {\n",
        "      \"action\": \"query\",\n",
        "      \"format\": \"json\",\n",
        "      \"list\": \"search\",\n",
        "      \"srsearch\": topic,\n",
        "      \"srlimit\": num_pages\n",
        "    }\n",
        "    try:\n",
        "      response = requests.get(self.base_url, params=search_params)\n",
        "      search_results = response.json()['query']['search']\n",
        "\n",
        "      for result in search_results:\n",
        "        content_params = {\n",
        "            \"action\": \"query\",\n",
        "            \"format\": \"json\",\n",
        "            \"prop\": \"extracts|info\",\n",
        "            \"pageids\": result['pageid'],\n",
        "            \"inprop\": \"url\",\n",
        "            \"explaintext\": True\n",
        "        }\n",
        "        content_response = requests.get(self.base_url, params=content_params)\n",
        "        page_data = content_response.json()['query']['pages'][str(result['pageid'])]\n",
        "        self.pages.append({\n",
        "          'id': result['pageid'],\n",
        "          'title': page_data['title'],\n",
        "          'url': page_data.get('fullurl', f\"https://en.wikipedia.org/?curid={result['pageid']}\"),\n",
        "          #'url': page_data.get('fullurl', f\"mqtt.org\"),\n",
        "          'content': page_data['extract']\n",
        "        })\n",
        "      print(f\"Retrieved: {page_data['title']}\")\n",
        "      return True\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"Error fetching pages: {str(e)}\")\n",
        "\n",
        "  def build_index(self):\n",
        "        \"\"\"Build a simple word location index\"\"\"\n",
        "        self.word_locations.clear()\n",
        "\n",
        "        # Process each page\n",
        "        for page in self.pages:\n",
        "            # Get all words from content\n",
        "            words = re.findall(r'\\w+', page['content'].lower())\n",
        "\n",
        "            # Count word frequencies\n",
        "            word_counts = defaultdict(int)\n",
        "            for word in words:\n",
        "                if word not in self.stop_words:\n",
        "                    word_counts[word] += 1\n",
        "\n",
        "            # Add to index with page information\n",
        "            for word, count in word_counts.items():\n",
        "                self.word_locations[word].append((page['id'], count))\n",
        "\n",
        "  def get_context(self, content, query_words, window=30):\n",
        "        \"\"\"Get a short snippet of text around the first matching word\"\"\"\n",
        "        content_lower = content.lower()\n",
        "        for word in query_words:\n",
        "            index = content_lower.find(word)\n",
        "            if index != -1:\n",
        "                start = max(index - window, 0)\n",
        "                end = min(index + window, len(content))\n",
        "                return content[start:end].strip()\n",
        "        return \"No context found.\"\n",
        "\n",
        "  def search(self, query, num_results=5):\n",
        "        \"\"\"Search pages using simple word frequency ranking.\n",
        "        Ranks pages based on:1. Number of query words found in the page\n",
        "        2. Total frequency of query words  \"\"\"\n",
        "        # Get query words\n",
        "        query_words = [word.lower() for word in re.findall(r'\\w+', query)\n",
        "                    if word.lower() not in self.stop_words]\n",
        "        if not query_words:\n",
        "            return []\n",
        "\n",
        "        # Calculate scores for each page\n",
        "        page_scores = defaultdict(lambda: {'matches': 0, 'total_freq': 0})\n",
        "\n",
        "        # For each query word\n",
        "        for word in query_words:\n",
        "            # Find pages containing this word\n",
        "            for page_id, freq in self.word_locations.get(word, []):\n",
        "                page_scores[page_id]['matches'] += 1\n",
        "                page_scores[page_id]['total_freq'] += freq\n",
        "\n",
        "\n",
        "        # Convert to list and sort\n",
        "        ranked_results = [\n",
        "            (page_id, scores['matches'], scores['total_freq'])\n",
        "            for page_id, scores in page_scores.items()\n",
        "        ]\n",
        "        # Sort by number of matching words first, then by total frequency\n",
        "        ranked_results.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
        "        # Format results\n",
        "        results = []\n",
        "        for page_id, matches, total_freq in ranked_results[:num_results]:\n",
        "            page = next(p for p in self.pages if p['id'] == page_id)\n",
        "            # Find the first matching word context\n",
        "            context = self.get_context(page['content'], query_words)\n",
        "            results.append({\n",
        "                'title': page['title'],\n",
        "                'url': page['url'],\n",
        "                'matching_words': matches,\n",
        "                'total_frequency': total_freq,\n",
        "                'context': context\n",
        "            })\n",
        "        return results\n"
      ],
      "metadata": {
        "id": "ElCWxwn2-h4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_wiki_pages(self, topic, num_pages=5):\n",
        "  \"\"\"Fetch Wikipedia pages for given topic\"\"\"\n",
        "  search_params = {\n",
        "  \"action\": \"query\",\n",
        "  \"format\": \"json\",\n",
        "  \"list\": \"search\",\n",
        "  \"srsearch\": topic,\n",
        "  \"srlimit\": num_pages\n",
        "  }\n",
        "  try:\n",
        "      response = requests.get(self.base_url, params=search_params)\n",
        "      search_results = response.json()['query']['search']\n",
        "\n",
        "      for result in search_results:\n",
        "        content_params = {\n",
        "            \"action\": \"query\",\n",
        "            \"format\": \"json\",\n",
        "            \"prop\": \"extracts|info\",\n",
        "            \"pageids\": result['pageid'],\n",
        "            \"inprop\": \"url\",\n",
        "            \"explaintext\": True\n",
        "        }\n",
        "        content_response = requests.get(self.base_url, params=content_params)\n",
        "        page_data = content_response.json()['query']['pages'][str(result['pageid'])]\n",
        "        self.pages.append({\n",
        "          'id': result['pageid'],\n",
        "          'title': page_data['title'],\n",
        "          'url': page_data.get('fullurl', f\"https://en.wikipedia.org/?curid={result['pageid']}\"),\n",
        "          'content': page_data['extract']\n",
        "        })\n",
        "        print(f\"Retrieved: {page_data['title']}\")\n",
        "      return True\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Error fetching pages: {str(e)}\")\n"
      ],
      "metadata": {
        "id": "ZhycCT9Q-mFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def build_index(self):\n",
        "        \"\"\"Build a simple word location index\"\"\"\n",
        "        self.word_locations.clear()\n",
        "\n",
        "        # Process each page\n",
        "        for page in self.pages:\n",
        "            # Get all words from content\n",
        "            words = re.findall(r'\\w+', page['content'].lower())\n",
        "\n",
        "            # Count word frequencies\n",
        "            word_counts = defaultdict(int)\n",
        "            for word in words:\n",
        "                if word not in self.stop_words:\n",
        "                    word_counts[word] += 1\n",
        "\n",
        "            # Add to index with page information\n",
        "            for word, count in word_counts.items():\n",
        "                self.word_locations[word].append((page['id'], count))\n",
        "\n",
        "    def search(self, query, num_results=5):\n",
        "        \"\"\"Search pages using simple word frequency ranking.\n",
        "        Ranks pages based on:1. Number of query words found in the page\n",
        "        2. Total frequency of query words  \"\"\"\n",
        "        # Get query words\n",
        "        query_words = [word.lower() for word in re.findall(r'\\w+', query)\n",
        "                      if word.lower() not in self.stop_words]\n",
        "        if not query_words:\n",
        "            return []\n",
        "\n",
        "        # Calculate scores for each page\n",
        "        page_scores = defaultdict(lambda: {'matches': 0, 'total_freq': 0})\n",
        "\n",
        "        # For each query word\n",
        "        for word in query_words:\n",
        "            # Find pages containing this word\n",
        "            for page_id, freq in self.word_locations.get(word, []):\n",
        "                page_scores[page_id]['matches'] += 1\n",
        "                page_scores[page_id]['total_freq'] += freq\n",
        "\n",
        "\n",
        "        # Convert to list and sort\n",
        "        ranked_results = [\n",
        "            (page_id, scores['matches'], scores['total_freq'])\n",
        "            for page_id, scores in page_scores.items()\n",
        "        ]\n",
        "        # Sort by number of matching words first, then by total frequency\n",
        "        ranked_results.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
        "        # Format results\n",
        "        results = []\n",
        "        for page_id, matches, total_freq in ranked_results[:num_results]:\n",
        "            page = next(p for p in self.pages if p['id'] == page_id)\n",
        "            # Find the first matching word context\n",
        "            context = self.get_context(page['content'], query_words)\n",
        "            results.append({\n",
        "                'title': page['title'],\n",
        "                'url': page['url'],\n",
        "                'matching_words': matches,\n",
        "                'total_frequency': total_freq,\n",
        "                'context': context\n",
        "            })\n",
        "        return results"
      ],
      "metadata": {
        "id": "7AxhkNvq-pMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# יצירת מופע של המנוע\n",
        "engine = WikiSearchEngine()\n",
        "\n",
        "# שליפת עמודים מוויקיפדיה (לדוגמה: נושא Python Programming)\n",
        "engine.fetch_wiki_pages(\"Python programming\", num_pages=5)\n",
        "\n",
        "# בניית אינדקס מילים\n",
        "engine.build_index()\n",
        "\n",
        "# ביצוע חיפוש\n",
        "results = engine.search(\"dynamic typing language\")\n",
        "\n",
        "# הצגת התוצאות\n",
        "for i, result in enumerate(results, 1):\n",
        "    print(f\"Result {i}:\")\n",
        "    print(f\"Title: {result['title']}\")\n",
        "    print(f\"URL: {result['url']}\")\n",
        "    print(f\"Matching Words: {result['matching_words']}\")\n",
        "    print(f\"Total Frequency: {result['total_frequency']}\")\n",
        "    print(f\"Context: {result['context']}\")\n",
        "    print(\"-\" * 60)\n"
      ],
      "metadata": {
        "id": "AoTJpFhNeAhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# יצירת מופע של המנוע\n",
        "engine = WikiSearchEngine()\n",
        "\n",
        "# שליפת עמודים מוויקיפדיה (לדוגמה: נושא Python Programming)\n",
        "engine.fetch_wiki_pages(\"software\", num_pages=5)\n",
        "\n",
        "# בניית אינדקס מילים\n",
        "engine.build_index()\n",
        "\n",
        "# ביצוע חיפוש\n",
        "results = engine.search(\"dynamic typing language\")\n",
        "\n",
        "# הצגת התוצאות\n",
        "for i, result in enumerate(results, 1):\n",
        "    print(f\"Result {i}:\")\n",
        "    print(f\"Title: {result['title']}\")\n",
        "    print(f\"URL: {result['url']}\")\n",
        "    print(f\"Matching Words: {result['matching_words']}\")\n",
        "    print(f\"Total Frequency: {result['total_frequency']}\")\n",
        "    print(f\"Context: {result['context']}\")\n",
        "    print(\"-\" * 60)\n"
      ],
      "metadata": {
        "id": "1DGqYZLWvlNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "class MQTTSimpleSearch:\n",
        "    def __init__(self):\n",
        "        self.url = \"https://mqtt.org/\"\n",
        "        self.text = \"\"\n",
        "        self.word_index = defaultdict(int)\n",
        "        self.stop_words = {'a', 'an', 'the', 'and', 'or', 'in', 'on', 'at', 'to', 'for', 'of', 'with'}\n",
        "\n",
        "    def fetch_site(self):\n",
        "        try:\n",
        "            response = requests.get(self.url)\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            self.text = soup.get_text().lower()\n",
        "            print(\"Site content fetched.\")\n",
        "        except Exception as e:\n",
        "            print(\"Error fetching site:\", str(e))\n",
        "\n",
        "    def build_index(self):\n",
        "        words = re.findall(r'\\w+', self.text)\n",
        "        for word in words:\n",
        "            if word not in self.stop_words:\n",
        "                self.word_index[word] += 1\n",
        "        print(\"Index built with\", len(self.word_index), \"unique words.\")\n",
        "\n",
        "    def search(self, query):\n",
        "        query_words = [word.lower() for word in re.findall(r'\\w+', query)\n",
        "                       if word.lower() not in self.stop_words]\n",
        "        results = {word: self.word_index[word] for word in query_words if word in self.word_index}\n",
        "        return results\n"
      ],
      "metadata": {
        "id": "ytgRrKUu1xcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "engine = MQTTSimpleSearch()\n",
        "engine.fetch_site()\n",
        "engine.build_index()\n",
        "print(engine.search(\"connect protocol broker\"))\n"
      ],
      "metadata": {
        "id": "A2Dgp95I1yjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# רשימת stop words בסיסית (ניתן לשנות בהתאם לצורך)\n",
        "stop_words = {\n",
        "    'a', 'an', 'the', 'in', 'on', 'at', 'for', 'to', 'from', 'is', 'are',\n",
        "    'of', 'and', 'or', 'by', 'with', 'that', 'this', 'it', 'as', 'be', 'was', 'were'\n",
        "}\n",
        "\n",
        "# פונקציה לניקוי תוכן הדף\n",
        "def clean_text(text):\n",
        "    words = re.findall(r'\\w+', text.lower())\n",
        "    return [word for word in words if word not in stop_words]\n",
        "\n",
        "# פונקציה לשליפת הקישורים הראשיים מתוך אתר mqtt.org\n",
        "def get_mqtt_links(base_url=\"https://mqtt.org/\"):\n",
        "    response = requests.get(base_url)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    links = soup.find_all('a', href=True)\n",
        "    valid_links = set()\n",
        "    for link in links:\n",
        "        href = link['href']\n",
        "        if href.startswith('/') or href.startswith(base_url):\n",
        "            if not href.startswith('mailto:'):\n",
        "                full_url = href if href.startswith('http') else base_url.rstrip('/') + '/' + href.lstrip('/')\n",
        "                valid_links.add(full_url)\n",
        "    return list(valid_links)\n",
        "\n",
        "# בניית האינדקס\n",
        "def build_index(links):\n",
        "    index = defaultdict(set)\n",
        "    for i, url in enumerate(links):\n",
        "        try:\n",
        "            resp = requests.get(url, timeout=5)\n",
        "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "            text = soup.get_text()\n",
        "            words = clean_text(text)\n",
        "            for word in words:\n",
        "                index[word].add(i)  # ניתן גם לשמור את ה-URL המלא במקום מספר מזהה\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to fetch {url}: {e}\")\n",
        "    return index\n"
      ],
      "metadata": {
        "id": "UqdrncSk2a3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# שלב ראשון: שליפת קישורים\n",
        "mqtt_links = get_mqtt_links()\n",
        "\n",
        "# שלב שני: בניית האינדקס\n",
        "mqtt_index = build_index(mqtt_links)\n",
        "\n",
        "# שלב שלישי: הדפסת אינדקס לדוגמה (20 מונחים ראשונים)\n",
        "for i, (term, doc_ids) in enumerate(mqtt_index.items()):\n",
        "    print({\"term\": term, \"DocIDs\": list(doc_ids)})\n",
        "    if i > 20:\n",
        "        break\n"
      ],
      "metadata": {
        "id": "yLQsnyXv28ZR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}