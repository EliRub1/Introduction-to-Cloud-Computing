{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN00QdSkLHSHujrGxiEPQf5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EliRub1/Introduction-to-Cloud-Computing/blob/main/ItCC_Tut10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnNcoxJGnuJM"
      },
      "outputs": [],
      "source": [
        "# Install Java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Download the latest Apache Spark version\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.4.1-bin-hadoop3.tgz\n",
        "\n",
        "# Install findspark to connect Python with Spark\n",
        "!pip install -q findspark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the os module to interact with the operating system\n",
        "import os\n",
        "# Import findspark to locate the Spark installation\n",
        "import findspark\n",
        "\n",
        "# Set the environment variable for Java home directory (required for Spark to run)\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# Set the environment variable for Spark home directory to the downloaded Spark path\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
        "\n",
        "# Initialize findspark to make pyspark importable within Python\n",
        "findspark.init()\n"
      ],
      "metadata": {
        "id": "R6Q0NIAqnxFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import SparkSession class from PySpark SQL module\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession object, which is the entry point to use Spark functionality\n",
        "  # Set the name of the Spark application to be \"Big Data Example\"\n",
        "  # Create a new SparkSession or return an existing one\n",
        "spark = SparkSession.builder.appName(\"Big Data Example\").getOrCreate()"
      ],
      "metadata": {
        "id": "u3BeK1cnnxDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of tuples, each containing a name and a price\n",
        "data = [(\"Tal\", 120), (\"Uri\", 90), (\"Dina\", 150)]\n",
        "# Define the column names for the DataFrame\n",
        "columns = [\"name\", \"price\"]\n",
        "# Create a DataFrame from the data and column names using the SparkSession\n",
        "df = spark.createDataFrame(data, columns)\n",
        "# Filter the DataFrame to include only rows where the price is greater than 100\n",
        "df.filter(df[\"price\"] > 100).show()\n"
      ],
      "metadata": {
        "id": "Mw7nOhrznxA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"datafiniti/consumer-reviews-of-amazon-products\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "HSEZ8R7bnw-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(\"/kaggle/input/consumer-reviews-of-amazon-products\", header=True, inferSchema=True)\n",
        "df.printSchema()\n",
        "df.show(50)"
      ],
      "metadata": {
        "id": "qeGCRjQTnw7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter positive reviews\n",
        "positive_reviews = df.filter(df[\"`reviews.rating`\"] >= 4)\n",
        "\n",
        "# Show examples of positive reviews\n",
        "positive_reviews.select(\"`reviews.text`\", \"`reviews.rating`\").show(50, truncate=False)\n"
      ],
      "metadata": {
        "id": "Ghw2iaWEnw5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count how many reviews there are per numerical rating\n",
        "df.filter(df[\"`reviews.rating`\"] >= 0).groupBy(\"`reviews.rating`\").count().orderBy(\"count\", ascending=False).show()"
      ],
      "metadata": {
        "id": "maeZy8a_nw2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Map Reducer"
      ],
      "metadata": {
        "id": "feAd698joGVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logs = [\n",
        "    \"192.168.1.10 - - [10/May/2025:13:00] GET /index.html\",\n",
        "    \"172.16.0.5 - - [10/May/2025:13:01] GET /contact.html\",\n",
        "    \"192.168.1.10 - - [10/May/2025:13:02] GET /products.html\",\n",
        "    \"10.0.0.1 - - [10/May/2025:13:02] GET /index.html\",\n",
        "    \"192.168.1.10 - - [10/May/2025:13:03] GET /about.html\"\n",
        "]"
      ],
      "metadata": {
        "id": "5KoGyIPDnwz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"IP Visit Count\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create RDD (Resilient Distributed Dataset) from the list of logs\n",
        "rdd = spark.sparkContext.parallelize(logs)\n",
        "\n",
        "# Map step: extract the IP and map each one to (IP, 1)\n",
        "ip_counts = rdd.map(lambda line: (line.split()[0], 1))\n",
        "\n",
        "# Reduce step: sum all counts per IP\n",
        "result = ip_counts.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Collect the results to the driver and print\n",
        "for ip, count in result.collect():\n",
        "    print(f\"{ip} visited {count} times\")\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "ZKfalDXAoJEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add = lambda x, y: x + y\n",
        "print(add(2, 3))"
      ],
      "metadata": {
        "id": "BW6kyZj-oI-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Start SparkSession\n",
        "spark = SparkSession.builder.appName(\"RDD Example\").getOrCreate()\n",
        "\n",
        "rdd = spark.sparkContext.parallelize([(\"apple\", 1), (\"banana\", 1), (\"apple\", 1)])\n",
        "result = rdd.reduceByKey(lambda a, b: a + b)\n",
        "print(result.collect())\n"
      ],
      "metadata": {
        "id": "_kEML9rKoI7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Word Count with MapReduce in PySpark:*"
      ],
      "metadata": {
        "id": "UffWkEZ-oQyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Start Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Big Data Word Count with MapReduce\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load text data (simulating a large dataset with parallelize)\n",
        "text = [\n",
        "    \"Big data is transforming the world\",\n",
        "    \"Apache Spark is fast and powerful\",\n",
        "    \"Big data requires scalable tools\",\n",
        "    \"Spark is designed for big data processing\"\n",
        "]\n",
        "\n",
        "# Create RDD from text lines\n",
        "rdd = spark.sparkContext.parallelize(text)\n",
        "\n",
        "# MapReduce steps:\n",
        "# Step 1: Split each line into words\n",
        "words = rdd.flatMap(lambda line: line.lower().split())\n",
        "\n",
        "# Step 2: Map each word to (word, 1)\n",
        "word_pairs = words.map(lambda word: (word, 1))\n",
        "\n",
        "# Step 3: Reduce by key (sum counts for each word)\n",
        "word_counts = word_pairs.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Collect and display the results\n",
        "for word, count in word_counts.collect():\n",
        "    print(f\"{word}: {count}\")\n"
      ],
      "metadata": {
        "id": "VPqKKO47oXp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Map Reduce analysis Big Data*"
      ],
      "metadata": {
        "id": "L1_0T-ssoamI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"datafiniti/consumer-reviews-of-amazon-products\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "R8tGWX4AofX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import isnull # Import the isnull function\n",
        "\n",
        "df = spark.read.csv(\"/kaggle/input/consumer-reviews-of-amazon-products\", header=True, inferSchema=True)\n",
        "# Select the review column and filter out rows where 'reviews.text' is null\n",
        "df_clean = df.select(col(\"`reviews.text`\").alias(\"review\")).filter(col(\"review\").isNotNull())\n",
        "# Create RDD from text lines\n",
        "rdd = df_clean.select(\"review\").rdd.map(lambda row: row[\"review\"])\n",
        "# MapReduce steps:\n",
        "# Step 1: Split each line into words\n",
        "words = rdd.flatMap(lambda line: line.lower().split())\n",
        "\n",
        "# Step 2: Map each word to (word, 1)\n",
        "word_pairs = words.map(lambda word: (word, 1))\n",
        "\n",
        "# Step 3: Reduce by key (sum counts for each word)\n",
        "word_counts = word_pairs.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Collect and display the results\n",
        "for word, count in word_counts.collect():\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "id": "OQp5TK_ZoiCH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}